library(ElemStatLearn)
data(SAheart)
View(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
fix(train)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
lm1 <- lm(chd ~ age + alcohol + obesity + tobacco + typea + ldl,data=SAheart)
summary(lm1)
glm1 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl,data=SAheart)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA,glm1)
predict <- predict(glm1)
missClass(testSA,predict)
missClass(trainSA,predict)
glm1 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl,data=trainSA)
predict <- predict(glm1)
missClass(trainSA,predict)
missClass(testSA,predict)
predict <- predict(glm1,newData=testSA)
missClass(trainSA,predict)
predict <- predict(glm1,newData=testSA$chd)
missClass(trainSA,predict)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
glm1 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl,data=trainSA,family="binomial")
prTr <- predict(glm1,trainSA)
prTs <- predict(glm1,testSA,type="response")
str(prTr)
prTr
str(prTr)
str(prTs)
View(testSA)
View(trainSA)
prTs2 <- predict(glm1,testSA)
str(prTs2)
str(prTs)
missClass(trainSA$chd, prTr)
missClass(testSA$chd, prTs)
lm3 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, family=binomial)
summary(lm3)
missClass(trainSA$chd, predict(lm3, type = "response"))
missClass(testSA$chd, predict(lm3, newdata=testSA, type = "response"))
set.seed(53535)
xValues = seq(0,2*pi,length=100)
yValues = rnorm(100) + sin(xValues)
library(splines)
xValues
yValues
plot(yValues ~ xValues)
ns1 <- ns(xValues,df=1)
ns1
par(mfrow=c(1,3))
plot(xValues,ns1[,1]); plot(xValues,ns1[,2]); plot(xValues,ns1[,3])
lm1 <- lm(yValues ~ ns1)
summary(lm1)
plot(xValues,yValues,pch=19,cex=0.1)
points(xValues,lm1$fitted,col="blue",pch=19,cex=0.5)
ns2 <- ns(xValues,df=2)
lm2 <- lm(yValues ~ ns2)
summary(lm2)
plot(xValues,yValues,pch=19,cex=0.1)
points(xValues,lm2$fitted,col="blue",pch=19,cex=0.5)
ns3 <- ns(xValues,df=3)
lm3 <- lm(yValues ~ ns3)
summary(lm3)
plot(xValues,yValues,pch=19,cex=0.1)
points(xValues,lm3$fitted,col="blue",pch=19,cex=0.5)
plot(xValues,yValues,pch=19,cex=0.1)
points(xValues,lm2$fitted,col="blue",pch=19,cex=0.5)
ns4 <- ns(xValues,df=4)
lm4 <- lm(yValues ~ ns4)
summary(lm4)
plot(xValues,yValues,pch=19,cex=0.1)
points(xValues,lm4$fitted,col="blue",pch=19,cex=0.5)
ns5 <- ns(xValues,df=5)
lm5 <- lm(yValues ~ ns5)
summary(lm5)
plot(xValues,yValues,pch=19,cex=0.1)
points(xValues,lm5$fitted,col="blue",pch=19,cex=0.5)
plot(xValues,yValues,pch=19,cex=0.1)
points(xValues,lm3$fitted,col="blue",pch=19,cex=0.5)
library(simpleboot)
data(airquality)
attach(airquality)
install.packages("simpleboot")
library(simpleboot)
data(airquality)
attach(airquality)
View(airquality)
summary(airquality$Wind)
seed(883833)
set.seed(883833)
bs <- function(data, indices,formula) {
d <- data[indices,];fit <- lm(formula, data=d);return(coef(fit))
}
results <- one.boot(data=airquality$Wind, statistic=bs, R=1000)
results <- one.boot(data=airquality$Wind, mean, statistic=bs, R=1000)
results
results$t[,2]
results$t[,1]
boot.ci(results)
plot(density(results$t[,2]),lwd=3,col="blue")
lines(rep(coef(nuke.lm)[2],10),seq(0,3,length=10),col="red",lwd=3)
results <- one.boot(data=airquality$Wind, mean, R=1000)
results <- one.boot(data=airquality$Wind, bs, R=1000)
results <- one.boot(data=airquality$Wind, mean, R=1000)
results <- one.boot(data=airquality, mean, R=1000)
results <- one.boot(data=airquality$Wind, mean, R=1000)
summary(results)
plot(density(results$t[,1]),lwd=3,col="blue")
lines(rep(coef(nuke.lm)[2],10),seq(0,3,length=10),col="red",lwd=3)
data(Cars93,package="MASS")
set.seed(7363)
set.seed(3343)
pValues = rep(NA,100)
for(i in 1:100){
z = rnorm(20)
x = rnorm(20)
y = rnorm(20,mean=0.5*x)
pValues[i] = summary(lm(y ~ x))$coef[2,4]
}
pValues
summary(lm(y ~ x))
sum(pValues < 0.1)
sum(p.adjust(pValues,method="bonferroni") < 0.1)
sum(p.adjust(pValues,method="BH") < 0.1)
rm(SAheart)
rm(olive)
rm(testSA)
rm(trainSA)
rm(glm1)
rm(lm1)
rm(lm3)
rm(prTr)
rm(prTs)
rm(prTs2)
rm(predict)
rm(train)
rm(missClass)
train <- read.csv("C:/MyDocs/titanik/1/train.csv")
View(train)
train.Age1 <- as.integer(round(train.Age))
train$Age1 <- as.integer(round(train$Age))
View(train)
rm(train$Age1)
train <- read.csv("C:/MyDocs/titanik/1/train.csv")
View(train)
train$Age <- as.integer(round(train$Age))
View(train)
train <- train[, c("Age1")]
install.packages("tree")
library("tree", lib.loc="C:/Users/Merle/Documents/R/win-library/2.15")
tree1 <- tree(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train)
summary(tree1)
rm(tree1)
train.tree <- tree(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train)
summary(train.tree)
plot(train.tree)
text(train.tree)
test <- read.csv("C:/MyDocs/titanik/1/test.csv")
View(test)
test$Age <- as.integer(round(test$Age))
View(test)
library(randomForest)
install.packages("randomForest")
library(randomForest)
train.rf <- randomForest(Survived ~ Pclass + Age + Sex + SibSp + Parch + Fare + Embarked, data=train, mtry=3, ntree=100, importance=TRUE)
train.rf <- randomForest(Survived ~ Pclass + Sex + Embarked, data=train, mtry=3, ntree=100, importance=TRUE)
summary(train.rf)
fix(train.rf)
set.seed(333)
x <- norm(30)
x <- rnorm(30)
x
plot(x)
scatterplot(x)
plot(density(x))
bootMean <- rep(NA,1000)
bootMean
sampleMean <- rep(NA,1000)
sample(x,replace=TRUE)
mean(sample(x,replace=TRUE))
mean(sample(x,replace=TRUE))
mean(sample(x,replace=TRUE))
?sample
for(i in 1:1000){bootMean[i] <- mean(sample(x,replace=TRUE))}
bootMean
for(i in 1:1000){sampledMean[i] <- mean(rnorm(30))}
for(i in 1:1000){sampleMean[i] <- mean(rnorm(30))}
sampleMean
plot(density(bootMean))
lines(density(sampleMean),col="red")
fix(x)
load("C:\Users\Merle\Downloads\5.R.RData")
load("C:\\Users\\Merle\\Downloads\\5.R.RData")
View(Xy)
matplot(Xy,type="l")
lm1 <- lm(y ~ X1 + X2, Xy)
summary(lm1)
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
}
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha.fn=function{}
alpha.fn=function(data,index){
with(data[index,],alpha(X,Y))
}
boot.out=boot(Xy,alpha.fn,R=1000)
library("boot", lib.loc="C:/Program Files/R/R-2.15.3/library")
boot.out=boot(Xy,alpha.fn,R=1000)
alpha.fn=function(data,index){
with(data[index,],alpha(X1,Y))
}
boot.out=boot(Xy,alpha.fn,R=1000)
alpha.fn=function(data,index){
with(data[index,],alpha(X1,y))
}
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
?boot
alpha=function(x,y){
vx=var(x)
vy=var(y)
cxy=cov(x,y)
(vy-cxy)/(vx+vy-2*cxy)
}
alpha(Xy$X1,Xy$y)
alpha(Xy$X2,Xy$y)
alpha.fn=function(data, index){
with(data[index,],alpha(X,Y))
}
dim(Xy)
alpha.fn(Xy,1:1000)
alpha.fn=function(data, index){
with(data[index,],alpha(X1,y))
}
alpha.fn(Xy,1:1000)
alpha.fn(Xy,1:100)
alpha.fn(Xy,1:900)
alpha.fn(Xy,sample(1:1000,100,replace=TRUE))
boot.out=boot(Portfolio,alpha.fn,R=1000)
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
boot.out=boot(Xy,alpha.fn,R=100)
boot.out
b1.stderr=function(y,X1,X2){
Xy.fit<-glm(y~X1+X2)
summary(Xy.fit)$coefficients[2,2]
}
b1.stderr.fn=function(data, index){
with(data[index,],b1.stderr(y,X1,X2))
}
boot.out=boot(Xy,b1.stderr.fn,R=1000)
boot.out
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
set.seed(1)
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
boot.out=boot(Xy,alpha.fn,R=1000)
boot.out
plot(boot.out)
alpha<-function(d){ coef(glm(y~X1+X2,data=d))}
alpha.fn<-function(data,index){ alpha(data[index,]) }
boot.out<-boot(Xy,alpha.fn,R=1000)
boot.out
boot.out<-boot(Xy,alpha.fn,R=1000)
boot.out
plot(boot.out)
boot.out=tsboot(Xy,alpha.fn,R=1000,sim="fixed",l=100)
boot.out
load("C:/MyDocs/coursera/stat_learning/7.R.RData")
fix(boot.out)
fix(lm1)
plot(x,y)
lm2 = lm(y~x)
summary(lm2)
lm3 = lm(y ~ 1+x+I(x^2))
summary(lm3)
setwd("C:\\MyDocs\\kaggle\\forest")
load(".RData")
cols <- names(train)[12:55]
cols
cols <- names(train.orig)[12:55]
cols
library(autoencoder)
install.packages("autoencoder")
nl=3 ## number of layers (default is 3: input, hidden, output)
unit.type = "logistic" ## specify the network unit type, i.e., the unit
N.input = 44 ## number of units (neurons) in the input layer (one unit per pixel)
N.hidden = 4 ## number of units in the hidden layer
lambda = 0.0005 ## weight decay parameter
beta = 6 ## weight of sparsity penalty term
rho = 0.01 ## desired sparsity parameter
epsilon <- 0.001 ## a small parameter for initialization of weights
max.iterations = 100000 ## number of iterations
trainmatrix = data.matrix(train.orig[,cols], rownames.force = NA)
testmatrix = data.matrix(valid.orig[,cols], rownames.force = NA)
autoencoder.object <- autoencode(X.train=trainmatrix,X.test=testmatrix, nl=nl,N.hidden=N.hidden,
unit.type=unit.type,lambda=lambda,beta=beta,rho=rho,epsilon=epsilon,
optim.method="BFGS",max.iterations=max.iterations)
library(autoencoder)
autoencoder.object <- autoencode(X.train=trainmatrix,X.test=testmatrix, nl=nl,N.hidden=N.hidden,
unit.type=unit.type,lambda=lambda,beta=beta,rho=rho,epsilon=epsilon,
optim.method="BFGS",max.iterations=max.iterations)
install.packages("autoencoder")
install.packages("autoencoder")
